apiVersion: v1
kind: Template
metadata:
  name: selectorsyncset-template
objects:
- apiVersion: hive.openshift.io/v1
  kind: SelectorSyncSet
  metadata:
    generation: 1
    labels:
      managed.openshift.io/gitHash: c5d89fb
      managed.openshift.io/osd: 'true'
    name: osd-managed-prometheus-exporter-ebs-iops-reporter
  spec:
    clusterDeploymentSelector:
      matchLabels:
        api.openshift.com/managed: 'true'
        hive.openshift.io/cluster-platform: aws
    resourceApplyMode: Sync
    resources:
    - apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: sre-ebs-iops-reporter
        namespace: openshift-monitoring
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        name: sre-ebs-iops-reporter
        namespace: openshift-monitoring
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: edit
      subjects:
      - kind: ServiceAccount
        name: sre-ebs-iops-reporter
        namespace: openshift-monitoring
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: sre-allow-read-machine-info
      rules:
      - apiGroups:
        - machine.openshift.io
        resources:
        - machines
        verbs:
        - get
        - list
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        name: sre-ebs-iops-reporter-read-machine-info
        namespace: openshift-machine-api
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: sre-allow-read-machine-info
      subjects:
      - kind: ServiceAccount
        name: sre-ebs-iops-reporter
        namespace: openshift-monitoring
    - apiVersion: cloudcredential.openshift.io/v1
      kind: CredentialsRequest
      metadata:
        name: sre-ebs-iops-reporter-aws-credentials
        namespace: openshift-monitoring
      spec:
        providerSpec:
          apiVersion: cloudcredential.openshift.io/v1
          kind: AWSProviderSpec
          statementEntries:
          - action:
            - cloudwatch:ListMetrics
            - cloudwatch:GetMetricData
            effect: Allow
            resource: '*'
        secretRef:
          name: sre-ebs-iops-reporter-aws-credentials
          namespace: openshift-monitoring
    - apiVersion: v1
      data:
        main.py: "#!/usr/bin/env python\n\nfrom sets import Set\n\nimport argparse\n\
          import boto3\nimport botocore\nimport datetime\nimport logging\nimport os\n\
          import re\nimport time\n\nfrom prometheus_client import start_http_server,\
          \ Gauge, Counter\n\nEBS_IOPS = Gauge(\"ebs_iops_credits\",\n           \
          \      \"Percent of burstable IOPS credit available\", labelnames=['vol_id'])\n\
          \n# A list (implemented as a Set) of all active volumes.\nACTIVE_VOLUMES\
          \ = Set([])\n\n# Period in minutes from cloudwatch to request\n# See https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_GetMetricData.html\n\
          CLOUDWATCH_PERIOD = 5\n\nBOTO_ERRS = Counter('boto_exceptions', 'The total\
          \ number of boto exceptions')\n\n\ndef chunks(l, n):\n    \"\"\"\n    Chunks\
          \ up an array +l+ into chunks of +n+ size\n    Based on https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n\
          \    \"\"\"\n    for i in xrange(0, len(l), n):\n        yield l[i:i+n]\n\
          \n\ndef get_utcnow():\n    # Split out the request for UTC from the round_time\
          \ function, built-ins cannot be mocked.\n    timetoround = datetime.datetime.utcnow()\n\
          \    return timetoround\n\n\ndef round_time(timetoround, period):\n    \"\
          \"\"\n    Round the current date (in utc) to the nearest +period+ minutes.\
          \ \n    For example, if the current time is 16:23 and +period+ is 10 (minutes),\
          \ the\n    return time will be 16:20.\n    Based on:\n    https://stackoverflow.com/questions/3463930/how-to-round-the-minute-of-a-datetime-object-python\n\
          \    \"\"\"\n    t = timetoround\n    t += datetime.timedelta(minutes=period/2.0)\n\
          \    t -= datetime.timedelta(minutes=t.minute % period,\n              \
          \              seconds=t.second,\n                            microseconds=t.microsecond)\n\
          \    return t\n\n\ndef collect(aws):\n    \"\"\"\n    Collect the current\
          \ data from the AWS API\n    \"\"\"\n\n    # All the volumes CloudWatch\
          \ tells us it knows about (whether or not it has data)\n    volumes = Set([])\n\
          \n    # List of volumes that we've actually had data back for the API\n\
          \    seen_volumes = Set([])\n\n    # get the volume IDs and filter the results\
          \ to show only metrics that have had data points published in the past three\
          \ hours\n    volumePager = cw.get_paginator('list_metrics')\n    for p in\
          \ volumePager.paginate(MetricName='BurstBalance', Namespace='AWS/EBS'):\n\
          \        for v in p['Metrics']:\n            volumes.add(v['Dimensions'][0]['Value'])\n\
          \n    dimensionCriteria = []\n    for volume in list(volumes):\n       \
          \ dimensionCriteria.append({\n            'Id': re.sub(r'^vol-', \"vol_\"\
          , volume, 0),\n            'MetricStat': {\n                'Metric': {\n\
          \                    'Namespace': 'AWS/EBS',\n                    'MetricName':\
          \ 'BurstBalance',\n                    'Dimensions': [\n               \
          \         {\n                            'Name': 'VolumeId', 'Value': volume\n\
          \                        }\n                    ]\n                },\n\
          \                'Period': CLOUDWATCH_PERIOD * 36,\n                'Stat':\
          \ 'Average',\n                'Unit': 'Percent',\n            }\n      \
          \  }\n        )\n\n    # get data for all the volume IDs\n    volumeDataPager\
          \ = cw.get_paginator('get_metric_data')\n    timetoround = get_utcnow()\n\
          \    time_start = round_time(timetoround, CLOUDWATCH_PERIOD)\n    logging.debug(\"\
          Requesting from %s to %s with period of %d minutes\",\n                \
          \  time_start -\n                  (datetime.timedelta(minutes=CLOUDWATCH_PERIOD)\
          \ * 2),\n                  time_start-datetime.timedelta(minutes=CLOUDWATCH_PERIOD),\n\
          \                  CLOUDWATCH_PERIOD*60)\n\n    # We have to go in chunks\
          \ of 100 volumes (now, dimensionCriteria) otherwise\n    # Error: The collection\
          \ MetricDataQueries must not have a size greater than 100.\n    # This is\
          \ a limitation of the AWS CloudWatch API\n    for dimensionChunk in chunks(dimensionCriteria,\
          \ 100):\n        # If the period is 5 minutes and the time is now 09:39\
          \ request from\n        # 09:30 - 09:35\n        # The intent is to get\
          \ a full section of data; since 09:35-09:39 isn't a full period,\n     \
          \   # the data might be unreliable.\n        for response in volumeDataPager.paginate(\n\
          \            StartTime=time_start -\n                (datetime.timedelta(minutes=CLOUDWATCH_PERIOD)\
          \ * 2),\n            EndTime=time_start-datetime.timedelta(minutes=CLOUDWATCH_PERIOD),\n\
          \            MetricDataQueries=dimensionChunk,\n        ):\n           \
          \ for mdr in response['MetricDataResults']:\n                if len(mdr['Values'])\
          \ > 0:\n                    seen_volumes.add(mdr['Label'])\n           \
          \         ACTIVE_VOLUMES.add(mdr['Label'])\n                    EBS_IOPS.labels(vol_id=mdr['Label']).set(mdr['Values'][0])\n\
          \                    logging.debug(\"%s has Values\", mdr['Label'])\n  \
          \              else:\n                    logging.debug(\"%s has no Values\"\
          , mdr['Label'])\n\n    logging.debug(\"Have %d ACTIVE_VOLUMES, seen %d volumes,\
          \ total volumes from list_metrics %d\", len(\n        ACTIVE_VOLUMES), len(seen_volumes),\
          \ len(volumes))\n    for inactive_volume in ACTIVE_VOLUMES - seen_volumes:\n\
          \        logging.info(\"Removing vol_id='%s' from Prometheus \", inactive_volume)\n\
          \        EBS_IOPS.remove(inactive_volume)\n        ACTIVE_VOLUMES.remove(inactive_volume)\n\
          \n\nif __name__ == \"__main__\":\n    logging.basicConfig(\n        level=logging.INFO,\
          \ format='%(asctime)s %(levelname)s:%(name)s:%(message)s')\n\n    parser\
          \ = argparse.ArgumentParser(\n        description='Options for EBS IOPS\
          \ Exporter')\n    parser.add_argument('-p', '--aws-profile',\n         \
          \               help='Name of AWS credentials profile to use', required=False,\
          \ default=\"default\")\n    parser.add_argument('-r', '--aws-region', help='AWS\
          \ Region to use',\n                        required=False, default=\"us-east-1\"\
          )\n    args = vars(parser.parse_args())\n\n    # Preference order for the\
          \ AWS profile:\n    # 1. Environment variables (AWS_PROFILE)\n    # 2. Argument\
          \ to program (--aws-profile)\n    # 3. \"default\", if neither are specified\n\
          \n    if \"AWS_PROFILE\" in os.environ:\n        args['aws_profile'] = os.environ['AWS_PROFILE']\n\
          \n    # If AWS_CONFIG_FILE is set then use the region in that file.\n  \
          \  # Else use the passed value or default.\n    if \"AWS_CONFIG_FILE\" in\
          \ os.environ:\n        session = boto3.session.Session(profile_name=args['aws_profile'])\n\
          \        args['aws_region'] = session.region_name\n    else:\n        session\
          \ = boto3.session.Session(\n            profile_name=args['aws_profile'],\
          \ region_name=args['aws_region'])\n\n    logging.info(\"Started ebs-iops-reporter\
          \ with aws_profile=%s, aws_region=%s\",\n                 args['aws_profile'],\
          \ args['aws_region'])\n\n    cw = session.client('cloudwatch')\n\n    start_http_server(8080)\n\
          \    while True:\n        try:\n            collect(cw)\n        except\
          \ botocore.exceptions.ClientError as err:\n            BOTO_ERRS.inc()\n\
          \            logging.error(\"Caught boto error\")\n            logging.error('Error\
          \ Message: {}'.format(\n                err.response['Error']['Message']))\n\
          \            logging.error('Request ID: {}'.format(\n                err.response['ResponseMetadata']['RequestId']))\n\
          \            logging.error('Http code: {}'.format(\n                err.response['ResponseMetadata']['HTTPStatusCode']))\n\
          \        finally:\n            # Sleep for the interval\n            logging.info(\"\
          Going to sleep for %d seconds\", CLOUDWATCH_PERIOD*60)\n            time.sleep(CLOUDWATCH_PERIOD\
          \ * 60)\n"
        start.sh: "#!/bin/sh\n\nset -o allexport\n\nif [[ -d /config && -d /config/env\
          \ ]]; then\n  source /config/env/*\nfi\n\nexec /usr/bin/python /monitor/main.py\
          \ \"$@\""
      kind: ConfigMap
      metadata:
        creationTimestamp: null
        name: sre-ebs-iops-reporter-code
        namespace: openshift-monitoring
    - apiVersion: apps.openshift.io/v1
      kind: DeploymentConfig
      metadata:
        labels:
          name: sre-ebs-iops-reporter
        name: sre-ebs-iops-reporter
        namespace: openshift-monitoring
      spec:
        paused: false
        replicas: 1
        selector:
          name: sre-ebs-iops-reporter
        strategy:
          type: Recreate
        template:
          metadata:
            labels:
              name: sre-ebs-iops-reporter
            name: sre-ebs-iops-reporter
          spec:
            affinity:
              nodeAffinity:
                preferredDuringSchedulingIgnoredDuringExecution:
                - preference:
                    matchExpressions:
                    - key: node-role.kubernetes.io/infra
                      operator: Exists
                  weight: 1
            containers:
            - command:
              - /bin/sh
              - /monitor/start.sh
              env:
              - name: AWS_SHARED_CREDENTIALS_FILE
                value: /secrets/aws/credentials.ini
              - name: AWS_CONFIG_FILE
                value: /secrets/aws/config.ini
              - name: PYTHONPATH
                value: /openshift-python/packages:/support/packages
              image: quay.io/openshift-sre/managed-prometheus-exporter-base:0.1.3-5a0899dd
              imagePullPolicy: IfNotPresent
              livenessProbe:
                failureThreshold: 2
                httpGet:
                  path: /
                  port: 8080
                initialDelaySeconds: 420
                periodSeconds: 360
                timeoutSeconds: 240
              name: main
              ports:
              - containerPort: 8080
                protocol: TCP
              readinessProbe:
                httpGet:
                  path: /
                  port: 8080
                initialDelaySeconds: 3
                timeoutSeconds: 240
              volumeMounts:
              - mountPath: /monitor
                name: monitor-volume
                readOnly: true
              - mountPath: /config
                name: envfiles
                readOnly: true
              - mountPath: /secrets
                name: secrets
                readOnly: true
              workingDir: /monitor
            dnsPolicy: ClusterFirst
            initContainers:
            - command:
              - /usr/local/bin/init.py
              - -r
              - /secrets/aws/config.ini
              - -a
              - /rawsecrets/aws_access_key_id
              - -A
              - /rawsecrets/aws_secret_access_key
              - -o
              - /secrets/aws/credentials.ini
              - -c
              - /config/env/CLUSTERID
              image: quay.io/openshift-sre/managed-prometheus-exporter-initcontainer:v0.1.9-2019-03-28-4e558131
              name: setupcreds
              volumeMounts:
              - mountPath: /rawsecrets
                name: awsrawcreds
                readOnly: true
              - mountPath: /secrets
                name: secrets
              - mountPath: /config
                name: envfiles
            restartPolicy: Always
            serviceAccountName: sre-ebs-iops-reporter
            tolerations:
            - effect: NoSchedule
              key: node-role.kubernetes.io/infra
              operator: Exists
            volumes:
            - name: awsrawcreds
              secret:
                secretName: sre-ebs-iops-reporter-aws-credentials
            - emptyDir: {}
              name: secrets
            - emptyDir: {}
              name: envfiles
            - configMap:
                name: sre-ebs-iops-reporter-code
              name: monitor-volume
        triggers:
        - type: ConfigChange
    - apiVersion: v1
      kind: Service
      metadata:
        labels:
          name: sre-ebs-iops-reporter
        name: sre-ebs-iops-reporter
        namespace: openshift-monitoring
      spec:
        ports:
        - name: http-main
          port: 80
          protocol: TCP
          targetPort: 8080
        selector:
          name: sre-ebs-iops-reporter
        sessionAffinity: None
        type: ClusterIP
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      metadata:
        labels:
          k8s-app: sre-ebs-iops-reporter
          name: sre-ebs-iops-reporter
        name: sre-ebs-iops-reporter
        namespace: openshift-monitoring
      spec:
        endpoints:
        - honorLabels: true
          interval: 2m
          port: http-main
          scheme: http
          scrapeTimeout: 2m
          targetPort: 0
        jobLabel: sre-ebs-iops-reporter
        namespaceSelector: {}
        selector:
          matchLabels:
            name: sre-ebs-iops-reporter
parameters:
- name: IMAGE_TAG
  required: true
  value: latest
